# SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation 

Cheng-Chun Hsu ${ }^{1,2}$, Bowen Wen ${ }^{1}$, Jie Xu ${ }^{1}$, Yashraj Narang ${ }^{1}$, Xiaolong Wang ${ }^{1,3}$, Yuke Zhu ${ }^{1,2}$, Joydeep Biswas ${ }^{1,2}$, Stan Birchfield ${ }^{1}$


#### Abstract

We introduce SPOT, an object-centric imitation learning framework. The key idea is to capture each task by an object-centric representation, specifically the $\mathrm{SE}(3)$ object pose trajectory relative to the target. This approach decouples embodiment actions from sensory inputs, facilitating learning from various demonstration types, including both action-based and action-less human hand demonstrations, as well as cross-embodiment generalization. Additionally, object pose trajectories inherently capture planning constraints from demonstrations without the need for manually crafted rules. To guide the robot to execute the task, the object trajectory is used to condition a diffusion policy. We show improvement compared to prior work on RLBench simulated tasks. In realworld evaluation, using only eight demonstrations shot on an iPhone, our approach completed all tasks while fully complying with task constraints. Project page: https://nvlabs.github.io/ object_centric_diffusion


## I. Introduction

Learning from demonstrations is an effective method for acquiring complex robotic manipulation skills. This approach simplifies learning vision-based manipulation policies by training directly on demonstration data, thus avoiding the need for intricate hand-crafted rules or laborious robot selfexploration in real-world settings. However, creating a structural representation that captures task-relevant information, remains resilient to environmental perturbations, and generalizes across the robot embodiment remains a significant challenge.

Previous work [1, 19, 36] has utilized raw RGB inputs for end-to-end visuomotor policy training. However, the absence of explicit 3D representation makes it difficult to generalize to various viewing angles and out-of-distribution spatial configurations. To address these issues, RVT [15] and PerAct [41] use the entire 3D scene as input, represented through point clouds and 3D volumes. While these methods consider the overall 3D scene structure, they include excessive redundant information, reducing data efficiency and necessitating extensive demonstration data collection. Additionally, the tight coupling of observation and action pairs complicates deployment across robotic embodiments, camera setups, or learning from action-less data like human hand demonstrations.

Object-centric methods address these problems by extracting object-centric information from visual observation, such as object detection [64], 6D object poses [53], 2D or 3D flows [58, 60, 65], to serve as inputs for downstream policies. By leveraging visual foundation models, these methods exhibit impressive generalization capabilities
${ }^{1}$ NVIDIA. ${ }^{2}$ University of Texas at Austin. ${ }^{3}$ University of California San Diego.
![](https://cdn.mathpix.com/cropped/2025_01_24_66cf8e9638299397b517g-1.jpg?height=568&width=827&top_left_y=540&top_left_x=1099)

Fig. 1: We present SPOT, an imitation learning method that leverages object pose trajectories as an intermediate representation. Given the observation, our framework estimates the object's pose relative to the target, predicts its future path in $\mathrm{SE}(3)$, and derives an action plan accordingly. Our diffusion model is trained on demonstration trajectories extracted from videos without needing action data from the same embodiment.
in new environments and demonstrate strong data efficiency, often requiring only a minimal amount of demonstration. Nevertheless, most works do not model the holistic object pose trajectories or focus only on the last-inch manipulation policy. This oversight necessitates extra human effort to manually create rules for constrained planning, especially in tasks where intermediate actions are crucial for success. For example, in tasks like pouring water or serving a plate, the container must remain upright throughout to prevent spillage. Such constraints are non-trivial to automatically learn with existing object-centric frameworks.

To address these issues, we introduce SPOT, an objectcentric diffusion policy framework that combines diffusion policy and object-centric representation for manipulation. As shown in Fig. 1, it uses the $\operatorname{SE}(3)$ object-centric trajectory relative to the target as the representation. This approach allows us to decouple the embodiment action from sensory input, making it easier to learn from various types of demonstration data, whether they involve action-based or actionless human hand demonstrations. In particular, opposite to the standard diffusion policy $[7,61]$ which directly produces robot end-effector action, our diffusion model learns to synthesize target $\mathrm{SE}(3)$ object-centric pose trajectories along the horizon, conditioned on the current observed object pose. The training data is obtained by extracting such objectcentric information from demonstration videos. For robot execution, the action is computed from the task space to transport the object following the target synthesized trajectory. By recurrently switching between future trajectory
synthesis and trajectory following, we achieve closed-loop control with enhanced robustness under dynamic uncertainty, such as in-hand object motion during grasping or placement. Unlike previous methods that focus solely on the last inch of manipulation, our approach trains on the pose trajectory data of the complete horizon, eliminating the need for manually crafted global planning rules before the last-inch stage. It also leverages the data scaling law when large amounts of demonstration data are available while maintaining data efficiency in few-shot demonstration scenarios.

We evaluate the approach on RLBench [18], where it significantly outperforms in high-precision and long-horizon tasks in a challenging single camera setting. In real-world tasks, with just 8 demonstrations shot on an iPhone, the resulting policies complete all tasks while fully complying with task constraints. We will make all datasets and code available so others can reproduce and expand upon this work. In summary, our framework has the following desired properties:

- Flexibility on diverse demonstration data. Our framework can learn from action-based robotic demonstrations and action-less data from human demonstrations. This flexibility enables easier data collection using accessible setups, such as a hand-held iPhone, without the need for robotic arms, as our experiments demonstrate.
- Autonomy on complex task constraints. Our framework considers the entire sequential object pose trajectory. This approach allows it to automatically learn planning constraints from demonstrations, eliminating the need for manually crafted rules.
- Closed-loop feedback. Our diffusion model offers control over the horizon of action prediction and execution, enabling closed-loop control based on updated observations. This enhances robustness in dynamic situations, like when an object slips during manipulation or placement.
- Generalization. Our object-centric representation allows us to ignore the task-irrelevant information from the background and thus generalizes robustly to varying lighting conditions, scene setup, and sensory configurations.
- Language-conditioned multi-task policy. Our framework inherits the benefit from diffusion models such that it can condition on language or other modalities to deploy on various tasks with a single set of weights, with negligible performance change compared to per-task policy training.


## II. RELATED WORK

End-to-end Imitation Learning. Significant advancements have been achieved in end-to-end imitation learning [7, $11,12,14,41,61]$, which involves directly mapping raw sensory observations to action predictions. This field can be broadly divided into two categories: discriminative models and generative models. The discriminative model aims to directly regress the action output given the observations from the collected demonstration data [14, 41]. Representative work PerAct [41] encodes the RGBD image of the entire scene as a 3D volume. With additional language conditioning, it achieves promising results with a low number of
demonstrations. RVT [14, 15] further enhances its efficiency by projecting the 3D scene cloud into orthogonal virtual views. However, the discriminative model often struggles with challenging action prediction tasks with multimodal distributions, sequential actions, or high precision. Recent works [7, 23, 59, 61] leverage diffusion model to learn an action distribution and synthesize action conditioning on the raw sensory input. Despite promising results, this work is limited by the need for extensive demonstration data, limited cross-embodiment generalization, and difficulty adapting to new scenarios.
Imitation Learning from Video. Recent advancements have shifted from relying on explicit robot action data to learning from visual demonstrations, which can be performed by either humans or robots $[3,4,11,22,25,33,34,38,40$, 44, 53]. Some recent approaches [42, 49] extract object state information from video demonstrations and frame the manipulation problem as action replay via low-level visual servoing primitive. Although these methods show promise, reasoning solely over pick and place ignores intermediate actions, preventing applications to tasks where intermediate action needs to satisfy certain constraints. Other methods [26, 56,58 ] use optical flow or point tracking as intermediate representations from video demonstrations, with additional policy networks to output actions. However, these policies rely on robot action demonstrations tied to the same embodiment, restricting cross-embodiment transfer. Another line of work synthesizes videos of desired task executions and determines action by learning an inverse-dynamics model [11] or extracting target object poses using optical flow [24]. Instead of synthesizing videos, which can be computationally intensive and noisy, we directly generate the object pose trajectory end-to-end.
Object-Centric Manipulation. A line of research leverages object-centric representations to reason about visual scenes in a modular way to promote policies prioritizing taskrelevant factors while reducing the influence of irrelevant visual distractions [53, 63]. Varying forms of object centric representations have been explored, including object pose [30-32, 50, 53], object detection/segmentation [8, 48, 51, 64], point tracking [13, 17, 39, 46, 49, 58, 60, 62, 65], neural implicit field [20, 43, 47, 57], or object-centric latent features extracted from visual foundation models [9, 10]. Among these, it is common to combine object-centric perception with conventional planning [32], or only focus on learning complex policies for the last-inch manipulation [9, 48, 53]. Therefore, they typically rely on manually programmed planning constraints for certain tasks where intermediate sequential action becomes critical [13, 17, 28], and non-trivial to scale to multiple tasks under a single set of policy/hyper-parameters. Moreover, the typical treatment of a constant rigid transform between gripper and object results in open-loop execution and thus lacks reaction to dynamic uncertainties [13, 28, 46]. Our framework inherits the prominent generalization properties of object-centric representations by building upon the recent visual foundation models, including object pose estimation and segmentation.

We additionally automate the task learning for the complete episode to minimize human intervention by reasoning over sequential object pose trajectories. Closed-loop feedback during robot execution further enhances robustness to dynamic uncertainties when dealing with high-precision tasks.

## III. Problem Setup

We aim to learn a policy $\pi$ that maps the observation $O_{t}$ to the robot's action $A_{t: t+h}$, where $h$ is the prediction horizon. A handful of demonstrations are provided for learning, which can be performed by either a robot or a human and recorded as RGBD videos. The policy focuses on prehensile manipulation tasks involving rigid objects. The observation $O_{t}$ represents the robot's sensory data, including RGBD images. For the action space, we utilize positional control, meaning $A_{t} \in S E(3)$ is specified by end-effector poses. In the case of single-policy for multi-tasks, task descriptions are provided to specify task-relevant objects and desired goals to distinguish each task.

## IV. APPROACH

Fig. 2 illustrates our imitation learning approach that utilizes object pose trajectories as an intermediate representation to learn the manipulation policy. First, we extract object pose trajectories from demonstration videos, which can be performed by either a robot or a human. Next, with the extracted object pose trajectories, we train a diffusion model to generate future object trajectories conditioning on the current and past object poses. In the case of a single-policy multitask setting, the diffusion model can additionally condition on a language embedding extracted from the task description to distinguish each task. Finally, we explain how to transform the generated object trajectories into embodimentagnostic action plans for closed-loop manipulation. Unlike end-to-end imitation learning methods that tie the grasping pose with subsequent motion, our object-centric approach is independent of specific robot hardware or grasp pose. This flexibility enables our method to integrate seamlessly with any online or offline grasp selection technique.

## A. Demonstration Video Parsing

We opt for object pose trajectories as the object-centric representation, which depicts the full $\mathrm{SE}(3)$ state information of rigid objects throughout the episode. Unlike previous methods that derive $\mathrm{SE}(3)$ poses from dense point flow [49], which can introduce redundancy and noise, directly reasoning about object poses enhances accuracy, as demonstrated in our experiments. Additionally, the compact nature of object poses improves compatibility with diffusion model training and boosts learning efficiency. To learn a policy from the demonstration video, the key step is to extract task-relevant objects and their pose trajectories.

Without loss of generality, we consider a single-arm manipulation scenario where only one object is manipulated at any given time. As a result, the task involves a set of objects $V=\left\{v_{a}, v_{b}\right\}$, consisting of a graspable source object $v_{a}$ and a target object $v_{b}$. For example, in a water-pouring task, the
graspable object might be a kettle, and the target object might be a mug. Given the demonstration video with length $l$, the goal is to extract pose trajectory $\left\{\hat{T}_{a}\right\}_{0}^{l}$ and $\left\{\hat{T}_{b}\right\}_{0}^{l}$ where each corresponding to object $v_{a}$ and $v_{b}$. These trajectories describe the task-specific object-centric motions in 3D and thus implicitly encode the planning constraints throughout the task, including those intermediates, such as keeping the kettle upright to prevent spilling until it approaches the mug. In practice, we apply a zero-shot 6D object pose estimation method to extract the pose trajectory from the video (Sec. IVC) for both the source and target objects. We then treat the source and target objects as two nodes in a scene graph and convert all source object poses into the target object's frame $\hat{\tau}=\left\{\hat{T}_{a}^{b}\right\}_{0}^{l}$. This transforms multiple demonstration trajectories into a canonicalized space, regardless of their absolute configurations or backgrounds, allowing the demonstration data to be collected in various environments.

Training directly on the dense, continuous pose trajectory can be inefficient and unstable due to the noisy, shaky motions from human hands or random pauses in the video. To address this, we employ a simple heuristic strategy to subsample keyframes. Specifically, a frame is added if the relative velocity is zero (indicating a change in direction) or if it exceeds a certain distance threshold from the previous keyframe, measured in both translation and rotation. More details on keyframe selection can be found in [41].

## B. Object Trajectory Diffusion

Our method decouples observation-action pairs in visuomotor policy learning by employing object pose trajectory as an intermediate representation. We model $p\left(T_{t: t+h} \mid T_{t}\right)$ to infer a sequence of future object pose trajectory targeted for task accomplishment $T_{t: t+h} \in S E(3)$, from the current object pose $T_{t} \in S E(3)$.

Specifically, we use the extracted object trajectories to train a task-dependent model that can synthesize the object pose trajectory. Our framework is built upon diffusion policy [7], which leverages a denoising diffusion probabilistic model (DDPM) to capture the conditional action distribution $p\left(A_{t} \mid O_{t}\right)$ :

$$
\begin{equation*}
A_{t}^{k-1}=\alpha\left(A_{t}^{k}-\gamma \varepsilon_{\theta}\left(O_{t}, A_{t}^{k}, k\right)+\mathscr{N}\left(0, \sigma^{2} I\right)\right) \tag{1}
\end{equation*}
$$

The process starts with $A_{t}^{K}$ sampled from Gaussian noise and is iterated $K$ times. Here, $(\alpha, \gamma, \sigma)$ are the parameters of the denoising scheduler. This process is also called reverse process and detailed in previous work [16]. The score function $\varepsilon_{\theta}$ is trained by the loss function

$$
\begin{equation*}
\mathscr{L}=\operatorname{MSE}\left(\varepsilon_{k}, \varepsilon_{\theta}\left(O_{t}, A_{t}^{0}+\varepsilon_{k}, k\right)\right) \tag{2}
\end{equation*}
$$

In our framework, we model $p\left(T_{t+1} \mid T_{t}\right)$ as the primary component for policy learning. Rather than predicting future actions, we synthesize the object pose trajectory for task progression. Additionally, our diffusion process is conditioned on the historical pose until now rather than raw sensory input. To adapt this method to our framework, we replace the observation input $O_{t}$ with the current and past object pose $T_{t}$. Furthermore, to obtain the object pose trajectory, $A_{t}$ is replaced by the transformation $T_{t+1}$. Accordingly, we
![](https://cdn.mathpix.com/cropped/2025_01_24_66cf8e9638299397b517g-4.jpg?height=747&width=1673&top_left_y=166&top_left_x=215)

Fig. 2: Overview. During training, we extract object pose trajectories from demonstration RGBD videos (e.g., collected with an iPhone), which are independent of the embodiment. Using these extracted trajectories, we train a diffusion model to generate future object trajectories and determine task completion based on current and past poses. During task execution, the task-relevant object is constantly tracked, and its pose is forwarded to the trajectory diffusion model to predict the object's future trajectory in $\operatorname{SE}(3)$, which leads to task accomplishment. Finally, we convert the generated trajectories into embodiment-agnostic action plans for closed-loop manipulation.
modified Eq. (1) to:

$$
\begin{equation*}
T_{t}^{k-1}=\alpha\left(T_{t}^{k}-\gamma \varepsilon_{\theta}\left(T_{t}, T_{t+1}^{k}, k\right)+\mathscr{N}\left(0, \sigma^{2} I\right)\right) \tag{3}
\end{equation*}
$$

The training loss introduced from Eq. (2) now becomes:

$$
\begin{equation*}
\mathscr{L}=\operatorname{MSE}\left(\varepsilon_{k}, \varepsilon_{\theta}\left(T_{t}, T_{t+1}^{0}+\varepsilon_{k}, k\right)\right) \tag{4}
\end{equation*}
$$

where the rotation is represented as a quaternion.
Pose Feature Encoder. Inspired by previous work [7], we employ a lightweight MLP network to encode object poses into compact representations. This network features a threelayer MLP, LayerNorm [2] layers, and a projection head, which projects the final features into a 64-dimensional vector. End State Prediction. To control the gripper state upon task completion, we employ a separate MLP network as a binary classifier to indicate task completeness. The classifier takes the current object pose as input and determines whether the rollout has reached its end state. We assign 1 to the end state and 0 to all other states, supervised by binary crossentropy loss. During deployment, the gripper opens when the prediction is beyond a threshold of 0.95 .
Multi-task Language Conditioning. Thanks to the flexibility of diffusion models, we can effortlessly adapt to singlemodel multi-task settings by conditioning on other multimodal data, such as language. Specifically, task descriptions are fed into a pretrained CLIP [35] model to obtain sentence embeddings, following previous work [14, 15, 41]. These feature embeddings are then concatenated with each object pose as input to the model.

We use DDIM [45] as the noise scheduler. The scheduling is set to 100 timesteps during training and 10 timesteps during inference. The training epoch is 3000 , and the batch size is 128 for all the baselines.

## C. Closed-loop Policy Execution

To derive the closed-loop manipulation policy $\pi$ from our object-centric representation, we need two key connections:
one from observation to object pose, $O_{t} \mapsto T_{c a m}^{o b j}$, and another from pose trajectory to action plan, $T_{t: t+h} \mapsto A_{t: t+h}$. For the first mapping, we utilize off-the-shelf object pose tracking. For the second mapping, the action plan is derived from the synthesized object trajectory. The complete framework consists of three main components: Object Pose Tracking, which provides real-time object poses at each step; Trajectory synthesizing, which takes these poses as input and outputs the future object trajectory for task progression; and finally, Action plan generation, which derives the action plan from the synthesized object trajectory.
Object Pose Tracking. We utilize FoundationPose [54], a foundation model for 6D object pose estimation and tracking, which can be applied to novel objects in zero-shot. It tracks all the task-relevant $\mathrm{SE}(3)$ object poses throughout the policy rollout to provide the latest feedback to the controller. Besides RGBD observations, the tracker requires a 3D object model and a 2D detection for the first frame. The 3D object mesh model can be easily created using consumergrade devices. In practice, we use an iPhone with LiDAR to scan the object into a textured mesh. For 2D object detection, we employ YOLO-World [6], an open-vocabulary object detection model. It takes as input the RGB image of the scene and task-relevant object names as text prompts and outputs their 2D detections. We leverage ChatGPT to extract task-relevant object names from the task description automatically. Similar to the demonstration data processing, we convert the estimated source object pose into the target object's coordinate frame before feeding it into the trajectory diffusion model.
Trajectory synthesizing. During testing, we employ the trained diffusion model (Sec. IV-B) to generate future object trajectories targeted for task accomplishment. At each step, the model uses the previous (if the observation horizon is
greater than 1) and current object poses as inputs to predict future object pose trajectories. We aim to achieve longhorizon planning while remaining reactive to unexpected dynamic uncertainties, such as in-hand object slipping or moving target objects. To this end, we integrate the object pose trajectories synthesized by the diffusion model with receding horizon control [29] to ensure robust action execution. Specifically, the model forecasts future object pose trajectories for $N$ steps, from which we select $K$ steps to execute actions. By adjusting $K$ and alternating between future trajectory prediction and trajectory following, we achieve closed-loop control based on updated observations from the object pose tracker.
Action plan generation. The synthesized trajectory indicates the desired object pose to complete the task. To ensure the object achieves this pose, we need to convert the pose trajectory into an executable action plan. Specifically, we infer executable actions from the synthesized trajectory. Given the synthesized trajectory, we transform the future object poses into end effector subgoals using the transformation $T_{o b j}^{E E} \in S E(3)$ from the object pose to the end effector pose. Given the extrinsic camera calibration, we can derive the transformation $T_{o b j}^{E E}=T_{E E}^{c a m} \cdot T_{c a m}^{o b j} \in S E(3)$ from the object pose to the end effector pose. Consequently, the action can be derived from the predicted object pose by $A_{t: t+h}=$ $T_{o b j}^{E E} \cdot T_{t: t+h}$. The action plan is constructed by following these end-effector subgoals. The policy continues to run until the episode ends and changes the gripper state, as signaled by the end-state prediction.

## V. EXPERIMENTS

Our experiments aim to address the following research questions: 1) Is our pose trajectory representation effective for capturing task-related 3D information? 2) Can our objectcentric approach learn from action-less data and bridge the embodiment and environmental gap between training and testing? 3) Can our object-centric approach generate trajectories that comply with task-specific motion constraints? 4) Can we democratize the demonstration data collection with a minimal hardware setup?

## A. Experimental Setup

Simulation setup. We evaluate baselines on RLBench [18], a standard multi-task manipulation benchmark adopted by previous studies [14, 15, 23, 41]. The task and the robot are simulated using CoppelaSim [37]. A Franka Panda robot equipped with a parallel jaw gripper is utilized for all tasks. Each task is accompanied by a language description and includes 2 to 60 variations, where the task-relevant objects are in different initial/goal configurations or appearances. We chose 13 manipulation tasks, excluding non-prehensile tasks and tasks with articulated objects, as they are beyond the scope of this work. Following [15, 41], all methods are trained on the same set of 100 demonstrations and tested on 25 unseen task configurations.
Real-world setup. To validate learning from videos in unstructured settings without specific environmental or hard-
ware requirements, we recorded demonstration videos in everyday environments using consumer-grade devices. Each task demonstration was performed by a human without robotics hardware, and filmed in various locations such as the kitchen, living room, or office, ensuring no overlap with the robot's testing environment. This results in diverse visual backgrounds, lighting conditions, and camera setups that are different from those used during robot testing. The videos were captured with an iPhone equipped with RGB and LiDAR sensors, and all manipulations were performed by human hands. For each task, 8 demonstrations were collected for training, and 10 trials were conducted for each method during evaluation. All real-world experiments were conducted using the same hardware setup: a robot equipped with a Kinova Gen3 7 DoF arm and a forward-facing Intel Realsense 415 RGBD camera mounted at the left shoulder of the robot, which was extrinsically calibrated.

## B. Simulation Results

The evaluations are displayed in Table I. This comparison addresses question (1) by demonstrating the effectiveness of our method across a diverse set of tasks, particularly in the challenging single-view setting.
Implementation details. To collect demonstration trajectories, we use the opening or closing of the gripper as signals to start or terminate a trajectory corresponding to an episode. For multi-stage tasks, in addition to language embedding, the object trajectory diffusion model is additionally conditioned on a one-hot encoding of the task stage. The next task stage is automatically triggered given the predicted end state (Sec. IV-B). Our approach focuses on generating post-grasp trajectories, relying on pre-generated grasp poses to grasp the object. Subsequently, actions are executed based on a synthesized pose trajectory. For object pose tracking, we reconstruct the object mesh from the training demonstration sequence using BundleSDF [55] and obtain the first frame object mask with SAM6D [27] through RGBD observations and the object mesh.
Baselines. We compare with two state-of-the-art 3D-based imitation learning methods: RVT2 [14] and 3D Diffuser Actor [23]. Both are language-conditioned, multi-task policies. We use their officially released implementations for retraining on the set of considered tasks. Notably, RVT2 requires multi-camera setup ( 4 in this case), whereas 3D Diffuser Actor supports single-view, the same as our setup. Analysis. Our method achieves performance comparable to the multi-view approach RVT2, even with a single camera setting where occlusion often occurs. Additionally, our method excels in high-precision tasks such as Insert Peg and Place Cups, outperforming other baselines. By leveraging pose trajectory representation, our method captures the essential spatial relationship between the taskrelevant objects, as well as the intermediate transformations during the process. Furthermore, our method surpasses other baselines in long-horizon tasks like Stack Blocks and Stack Cups, as its object-centric representation naturally decomposes long-horizon, multi-object task learning into

TABLE I: Simulation Results on RLBench. All methods use a single set of weights to perform multiple tasks, conditioned on language description.

| Method | Single <br> View | Avg <br> Succ. | Close <br> Jar | Drag <br> Stick | Insert <br> Peg | Meat off <br> Grill | Place <br> Cups | Place <br> Wine |
| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| RVT2 [14] | $\boldsymbol{X}$ | 76.4 | $\mathbf{9 8 . 7} \pm \mathbf{2 . 3}$ | $\mathbf{9 8 . 7} \pm \mathbf{2 . 3}$ | $44.0 \pm 6.9$ | $96.0 \pm 0.0$ | $33.3 \pm 2.3$ | $92.0 \pm 4.0$ |
| 3D-DA $[23]$ | $\checkmark$ | 54.5 | $77.3 \pm 0.0$ | $92.0 \pm 8.0$ | $9.3 \pm 2.3$ | $84.0 \pm 0.0$ | $4.0 \pm 0.0$ | $78.7 \pm 4.6$ |
| Ours | $\checkmark$ | $\mathbf{7 9 . 4}$ | $\mathbf{9 8 . 7} \pm \mathbf{2 . 3}$ | $80.0 \pm 0.0$ | $\mathbf{7 8 . 7} \pm \mathbf{2 . 3}$ | $\mathbf{1 0 0 . 0} \pm \mathbf{0 . 0}$ | $\mathbf{6 2 . 7} \pm \mathbf{6 . 1}$ | $\mathbf{1 0 0 . 0} \pm \mathbf{0 . 0}$ |
|  | Single | Put in | Put in | Screw | Sort | Stack | Stack | Turn |
| Method | View | Cupboard | Safe | Bulb | Shape | Blocks | Cups | Tap |
| RVT2 [14] | $\boldsymbol{X}$ | $\mathbf{6 9 . 3} \pm \mathbf{2 . 3}$ | $89.3 \pm 8.3$ | $\mathbf{8 6 . 7} \pm \mathbf{4 . 6}$ | $\mathbf{4 9 . 3} \pm \mathbf{6 . 1}$ | $81.3 \pm 6.1$ | $60.0 \pm 0.0$ | $94.7 \pm 4.6$ |
| 3D-DA $[23]$ | $\checkmark$ | $38.7 \pm 2.3$ | $86.7 \pm 2.3$ | $38.7 \pm 4.6$ | $30.7 \pm 2.3$ | $41.7 \pm 7.5$ | $30.7 \pm 8.3$ | $96.0 \pm 0.0$ |
| Ours | $\checkmark$ | $42.7 \pm 4.6$ | $\mathbf{1 0 0 . 0} \pm \mathbf{0 . 0}$ | $48.0 \pm 8.0$ | $32.0 \pm 4.0$ | $\mathbf{9 4 . 0} \pm \mathbf{3 . 4}$ | $\mathbf{9 6 . 0} \pm \mathbf{0 . 0}$ | $\mathbf{1 0 0 . 0} \pm \mathbf{0 . 0}$ |

a series of sub-policies. However, our method struggles with thin objects (the stick in Drag Stick), small objects occluded by the gripper (the bulbs in Screw Bulb), or a mix of objects with different symmetries (the shape toys in Sort Shape).

## C. Results in Real-world

The evaluations for the four tasks are shown in Figure 4. This comparison addresses questions (2)-(4) by determining whether our method can transfer 3D trajectories captured in cross-embodiment videos (human demonstrations) and successfully meet the tasks' motion constraints through automatic learning under minimal data collection efforts.
Tasks and evaluation protocol. We designed four tasks to evaluate policy performance: 1) mug-on-coaster: placing a mug on a coaster; 2) plant-in-vase: inserting a cactus plant into a vase; 3) pour-water: using a kettle to pour water into a mug; 4) put-plate-into-oven: placing a plate with food into an oven. These four tasks involve various challenges commonly considered in robot manipulation. In particular, Task 1 represents the commonly considered pick-and-place task while requiring a reasonable level of precision due to the small size of the coaster. Task 2 requires even higher precision due to the low tolerance between the plant base and the vase. The last two tasks additionally require satisfying the object's intermediate motion constraints throughout the episode: the kettle and plate must stay upright, and the content should stay inside/on throughout the entire episode. Regarding the task outcome, other than task success, we use Placing failure to indicate that the robot fails to deliver the object to the goal position. Task constraint failure describes a scenario where the robot successfully delivers the object but fails to meet the task constraints. Tracking failure occurs when the object tracker loses track of the object, forcing the rollout to terminate early, as we rely on object tracking to infer the action plan.
Baselines. Learning policy from action-less video while adhering to task-specific constraints is challenging. Previous works [5, 49, 56, 60] exploit point tracking for extracting task-relevant actions from videos. Accordingly, we implement a Point-Tracking baseline, utilizing a heuristic point-tracking method to derive object trajectories from the video. Specifically, we uniformly sample key points from the first frame, track these points using the CoTracker [21], apply RANSAC to extract object transformations throughout the sequence, and use these transformations for learning
![](https://cdn.mathpix.com/cropped/2025_01_24_66cf8e9638299397b517g-6.jpg?height=646&width=825&top_left_y=604&top_left_x=1103)

Fig. 3: Real-world Tasks and Qualitative Results. Demonstration data was collected using an iPhone to record the RGBD video of a human performing the tasks (Left). The robot deploys the trained policy in drastically different environments, lighting conditions, camera perspectives, and object configurations from demonstration time (Right).
![](https://cdn.mathpix.com/cropped/2025_01_24_66cf8e9638299397b517g-6.jpg?height=570&width=827&top_left_y=1447&top_left_x=1099)

Fig. 4: Real-world Quantitative Results. Our method outperforms the point-tracking baseline. We categorize failure modes into (i) tracking failure, (ii) placing failure, and (iii) task constraint failure.
trajectory synthesis. This baseline allows us to compare our approach with existing methods in modeling intermediate states and implicit task constraints.
Analysis. Without specific task constraints, all evaluation methods perform well in the general pick-and-place task of mug-on-coaster. However, in the plant-in-vase and pour-water tasks, Point-Tracking struggles to achieve sufficient accuracy for the plant to fit into the vase due to the noisy extracted object trajectory. Additionally, the
object pose tracker sometimes fails to track the cactus plant in plant-in-vase due to the camera setup and occlusion caused by the robot arm, highlighting a shared limitation of both methods. In the put-plate-into-oven task, the robot must keep the plate upright while inserting it into the narrow space inside the oven. Our method outperforms the baseline by producing smooth trajectories while adhering to constraints. In contrast, Point-Tracking often drastically changes orientation, causing content to fall off the plate.

## VI. Conclusion

We present SPOT, an object-centric imitation learning method that allows robots to learn from cross-embodiment demonstrations and adhere to task constraints using an intermediate representation of object pose trajectories. Our approach showcases robust real-world manipulation capabilities, outperforming all baselines across various manipulation tasks. Despite its success, SPOT has limitations: 1) The learned policy cannot handle non-prehensile tasks. 2) Our approach relies on 6D pose tracking, which assume object is rigid and may struggle with small, thin, or materialless objects. 3) Object tracking requires reconstructed object mesh, though this has become more accessible with several recent advancements [52, 55].

## References

[1] M. Ahn et al., "Do as I can, not as I say: Grounding language in robotic affordances," arXiv preprint arXiv:2204.01691, 2022.
[2] J. Ba, "Layer normalization," arXiv preprint arXiv:1607.06450, 2016.
[3] S. Bahl, A. Gupta, and D. Pathak, "Human-to-robot imitation in the wild," in RSS, 2022.
[4] B. Baker et al., "Video pretraining (vpt): Learning to act by watching unlabeled online videos," Advances in Neural Information Processing Systems, vol. 35, pp. 24639-24654, 2022.
[5] H. Bharadhwaj, R. Mottaghi, A. Gupta, and S. Tulsiani, "Track2act: Predicting point tracks from internet videos enables diverse zeroshot robot manipulation," arXiv preprint arXiv:2405.01527, 2024.
[6] T. Cheng, L. Song, Y. Ge, W. Liu, X. Wang, and Y. Shan, "Yoloworld: Real-time open-vocabulary object detection," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 16901-16911.
[7] C. Chi et al., "Diffusion policy: Visuomotor policy learning via action diffusion," RSS, 2023.
[8] C. Devin, P. Abbeel, T. Darrell, and S. Levine, "Deep objectcentric representations for generalizable robot learning," in IEEE International Conference on Robotics and Automation (ICRA), 2018, pp. 7111-7118.
[9] N. Di Palo and E. Johns, "Dinobot: Robot manipulation via retrieval and alignment with vision foundation models," arXiv preprint arXiv:2402.13181, 2024.
[10] N. Di Palo and E. Johns, "Keypoint action tokens enable in-context imitation learning in robotics," arXiv preprint arXiv:2403.19578, 2024.
[11] Y. Du et al., "Learning universal policies via text-guided video generation," Advances in Neural Information Processing Systems, vol. 36, 2024.
[12] Z. Fu, T. Z. Zhao, and C. Finn, "Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation," CoRL, 2024.
[13] W. Gao and R. Tedrake, "Kpam 2.0: Feedback control for categorylevel robotic manipulation," IEEE Robotics and Automation Letters, vol. 6, no. 2, pp. 2962-2969, 2021.
[14] A. Goyal, V. Blukis, J. Xu, Y. Guo, Y.-W. Chao, and D. Fox, "Rvt2: Learning precise manipulation from few demonstrations," RSS, 2024.
[15] A. Goyal, J. Xu, Y. Guo, V. Blukis, Y.-W. Chao, and D. Fox, "Rvt: Robotic view transformer for 3d object manipulation," in Conference on Robot Learning, 2023, pp. 694-710.
[16] J. Ho, A. Jain, and P. Abbeel, "Denoising diffusion probabilistic models," Advances in neural information processing systems, vol. 33, pp. 6840-6851, 2020.
[17] W. Huang, C. Wang, Y. Li, R. Zhang, and L. Fei-Fei, "Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation," arXiv preprint arXiv:2409.01652, 2024.
[18] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, "Rlbench: The robot learning benchmark \& learning environment," IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 3019-3026, 2020.
[19] E. Jang et al., "Bc-z: Zero-shot task generalization with robotic imitation learning," in Conference on Robot Learning, PMLR, 2022, pp. 991-1002.
[20] S. Jauhri, I. Lunawat, and G. Chalvatzaki, "Learning any-view 6dof robotic grasping in cluttered scenes via neural surface rendering," arXiv preprint arXiv:2306.07392, 2023.
[21] N. Karaev, I. Rocco, B. Graham, N. Neverova, A. Vedaldi, and C. Rupprecht, "Cotracker: It is better to track together," arXiv preprint arXiv:2307.07635, 2023.
[22] H. Karnan, F. Torabi, G. Warnell, and P. Stone, "Adversarial imitation learning from video using a state observer," in International Conference on Robotics and Automation (ICRA), 2022, pp. 24522458.
[23] T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki, "3d diffuser actor: Policy diffusion with 3d scene representations," arXiv preprint arXiv:2402.10885, 2024.
[24] P.-C. Ko, J. Mao, Y. Du, S.-H. Sun, and J. B. Tenenbaum, "Learning to act from actionless videos through dense correspondences," ICLR, 2024.
[25] T. Kurutach, A. Tamar, G. Yang, S. J. Russell, and P. Abbeel, "Learning plannable representations with causal infogan," Advances in Neural Information Processing Systems, vol. 31, 2018.
[26] L.-H. Lin, Y. Cui, A. Xie, T. Hua, and D. Sadigh, "Flowretrieval: Flow-guided data retrieval for few-shot imitation learning," arXiv preprint arXiv:2408.16944, 2024.
[27] J. Lin, L. Liu, D. Lu, and K. Jia, "Sam-6d: Segment anything model meets zero-shot 6d object pose estimation," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 27906-27916.
[28] L. Manuelli, W. Gao, P. Florence, and R. Tedrake, "Kpam: Keypoint affordances for category-level robotic manipulation," in The International Symposium of Robotics Research, 2019, pp. 132-157.
[29] D. Q. Mayne and H. Michalska, "Receding horizon control of nonlinear systems," in Proceedings of the 27th IEEE Conference on Decision and Control, 1988, pp. 464-465.
[30] T. Migimatsu and J. Bohg, "Object-centric task and motion planning in dynamic environments," IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 844-851, 2020.
[31] A. S. Morgan, K. Hang, B. Wen, K. Bekris, and A. M. Dollar, "Complex in-hand manipulation via compliance-enabled finger gaiting and multi-modal planning," IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 4821-4828, 2022.
[32] A. S. Morgan, B. Wen, J. Liang, A. Boularias, A. M. Dollar, and K. Bekris, "Vision-driven compliant manipulation for reliable, highprecision assembly tasks," RSS, 2021.
[33] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, "R3m: A universal visual representation for robot manipulation," CoRL, 2022.
[34] J. Pari, N. M. Shafiullah, S. P. Arunachalam, and L. Pinto, "The surprising effectiveness of representation learning for visual imitation," RSS, 2022.
[35] A. Radford et al., "Learning transferable visual models from natural language supervision," in International Conference on Machine Learning, 2021, pp. 8748-8763.
[36] S. Reed et al., "A generalist agent," Transactions on Machine Learning Research, 2022.
[37] E. Rohmer, S. P. Singh, and M. Freese, "V-rep: A versatile and scalable robot simulation framework," in IEEE/RSJ International Conference on Intelligent Robots and Systems, 2013, pp. 1321-1326.
[38] H. Ryu, H.-i. Lee, J.-H. Lee, and J. Choi, "Equivariant descriptor fields: Se (3)-equivariant energy-based models for end-to-end visual robotic manipulation learning," ICLR, 2023.
[39] D. Seita, Y. Wang, S. J. Shetty, E. Y. Li, Z. Erickson, and D. Held, "Toolflownet: Robotic manipulation with tools via predicting tool
flow from point clouds," in Conference on Robot Learning, PMLR, 2023, pp. 1038-1049.
[40] P. Sharma, D. Pathak, and A. Gupta, "Third-person visual imitation learning via decoupled hierarchical controller," Advances in Neural Information Processing Systems, vol. 32, 2019.
[41] M. Shridhar, L. Manuelli, and D. Fox, "Perceiver-actor: A multitask transformer for robotic manipulation," in Conference on Robot Learning, PMLR, 2023, pp. 785-799.
[42] M. Sieb, Z. Xian, A. Huang, O. Kroemer, and K. Fragkiadaki, "Graph-structured visual imitation," in Conference on Robot Learning, PMLR, 2020, pp. 979-989.
[43] A. Simeonov et al., "Neural descriptor fields: Se (3)-equivariant object representations for manipulation," in 2022 International Conference on Robotics and Automation (ICRA), IEEE, 2022, pp. 63946400.
[44] A. Sivakumar, K. Shaw, and D. Pathak, "Robotic telekinesis: Learning a robotic hand imitator by watching humans on youtube," arXiv preprint arXiv:2202.10448, 2022.
[45] J. Song, C. Meng, and S. Ermon, "Denoising diffusion implicit models," arXiv preprint arXiv:2010.02502, 2020.
[46] P. Sundaresan, S. Belkhale, D. Sadigh, and J. Bohg, "Kite: Keypointconditioned policies for semantic manipulation," CoRL, 2023.
[47] J. Urain, N. Funk, J. Peters, and G. Chalvatzaki, "Se (3)diffusionfields: Learning smooth cost functions for joint grasp and motion optimization through diffusion," in 2023 IEEE International Conference on Robotics and Automation (ICRA), IEEE, 2023, pp. 5923-5930.
[48] E. Valassakis, G. Papagiannis, N. Di Palo, and E. Johns, "Demonstrate once, imitate immediately (dome): Learning visual servoing for one-shot imitation learning," in 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE, 2022, pp. 8614-8621.
[49] M. Vecerik et al., "Robotap: Tracking arbitrary points for fewshot visual imitation," in 2024 IEEE International Conference on Robotics and Automation (ICRA), IEEE, 2024, pp. 5397-5403.
[50] P. Vitiello, K. Dreczkowski, and E. Johns, "One-shot imitation learning: A pose estimation perspective," CoRL, 2023.
[51] D. Wang, C. Devin, Q.-Z. Cai, F. Yu, and T. Darrell, "Deep object-centric policies for autonomous driving," in 2019 International Conference on Robotics and Automation (ICRA), IEEE, 2019, pp. 8853-8859.
[52] S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud, "Dust3r: Geometric 3d vision made easy," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 20 697-20 709.
[53] B. Wen, W. Lian, K. Bekris, and S. Schaal, "You only demonstrate once: Category-level manipulation from single visual demonstration," RSS, 2022.
[54] B. Wen, W. Yang, J. Kautz, and S. Birchfield, "Foundationpose: Unified 6d pose estimation and tracking of novel objects," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 17 868-17 879.
[55] B. Wen et al., "Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 606-617.
[56] C. Wen et al., "Any-point trajectory modeling for policy learning," arXiv preprint arXiv:2401.00025, 2023.
[57] T. Weng, D. Held, F. Meier, and M. Mukadam, "Neural grasp distance fields for robot manipulation," in 2023 IEEE International Conference on Robotics and Automation (ICRA), IEEE, 2023, pp. 1814-1821.
[58] M. Xu et al., "Flow as the cross-domain manipulation interface," arXiv preprint arXiv:2407.15208, 2024.
[59] G. Yan, Y.-H. Wu, and X. Wang, "Dnact: Diffusion guided multitask 3d policy learning," arXiv preprint arXiv:2403.04115, 2024.
[60] C. Yuan, C. Wen, T. Zhang, and Y. Gao, "General flow as foundation affordance for scalable robot learning," arXiv preprint arXiv:2401.11439, 2024.
[61] Y. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, and H. Xu, "3d diffusion policy," arXiv preprint arXiv:2403.03954, 2024.
[62] H. Zhang, B. Eisner, and D. Held, "Flowbot++: Learning generalized articulated objects manipulation via articulation projection," CoRL, 2023.
[63] Y. Zhu, Z. Jiang, P. Stone, and Y. Zhu, "Learning generalizable manipulation policies with object-centric 3d representations," CoRL, 2023.
[64] Y. Zhu, A. Joshi, P. Stone, and Y. Zhu, "Viola: Imitation learning for vision-based manipulation with object proposal priors," in Conference on Robot Learning, PMLR, 2023, pp. 1199-1210.
[65] Y. Zhu, A. Lim, P. Stone, and Y. Zhu, "Vision-based manipulation from single human video with open-world object graphs," arXiv preprint arXiv:2405.20321, 2024.

